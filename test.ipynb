{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.utils import create_kanji_dataset, TrainingConfig\n",
    "# import torch\n",
    "\n",
    "dataset = create_kanji_dataset() # hf dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17444/17444 [01:05<00:00, 267.85it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f22ba5fb1d044b5ab031c2da78a4b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f98af5ee7947af8a1e7446058953e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17444 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c05701a94b4f4781f7b8a920ecc89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/175 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed6d5b26ca7401181e7c537bdd97bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f6a47b15f440d7a56624e4cbc32708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3752 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b6cdb22046402a92299172b317968e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/38 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Ksgk-fy/concept-kanji-dataset/commit/d5239864024c88edd66f0f30c40344da68508b7c', commit_message='Upload dataset', commit_description='', oid='d5239864024c88edd66f0f30c40344da68508b7c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/Ksgk-fy/concept-kanji-dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='Ksgk-fy/concept-kanji-dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the Special Token \"Kanji\" for DreamBooth Concept Tuning first, then use caption-image matching for Text2Image fune-tune\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"Ksgk-fy/expanded-kanji-dataset\")\n",
    "os.makedirs(\"data/kanji_expand_image\", exist_ok=True)\n",
    "\n",
    "# Use special concept token \n",
    "special_token = \"Kanji\"\n",
    "CAPTION_TEMPLATE = \"an image of {special_token} meaning {meaning}\"\n",
    "transform_text = lambda meaning: CAPTION_TEMPLATE.format(special_token=special_token, meaning=meaning)\n",
    "\n",
    "# Transform all text fields using the template\n",
    "dataset = dataset.map(lambda x: {'text': transform_text(x['text'])})\n",
    "\n",
    "# Save images\n",
    "for idx, img in enumerate(tqdm(dataset['train']['image'])):\n",
    "    if not os.path.exists(f\"data/kanji_expand_image/kanji_{idx}.png\"):\n",
    "        img.save(f\"data/kanji_expand_image/kanji_{idx}.png\")\n",
    "        \n",
    "dataset.push_to_hub(\"Ksgk-fy/concept-kanji-dataset\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset \n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch \n",
    "import os \n",
    "from pathlib import Path\n",
    "tokenize_prompt = lambda x: x\n",
    "\n",
    "class InceptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset to prepare the instance and class images with the prompts for fine-tuning the model.\n",
    "    It pre-processes the images and the tokenizes prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        instance_data_root,\n",
    "        instance_prompt,\n",
    "        tokenizer,\n",
    "        class_data_root=None,\n",
    "        class_prompt=None,\n",
    "        class_num=None,\n",
    "        size=512,\n",
    "        center_crop=False,\n",
    "        encoder_hidden_states=None,\n",
    "        class_prompt_encoder_hidden_states=None,\n",
    "        tokenizer_max_length=None,\n",
    "        dataset_name=None,\n",
    "        text_field=\"text\",\n",
    "    ):\n",
    "        self.size = size\n",
    "        self.center_crop = center_crop\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoder_hidden_states = encoder_hidden_states\n",
    "        self.class_prompt_encoder_hidden_states = class_prompt_encoder_hidden_states\n",
    "        self.tokenizer_max_length = tokenizer_max_length\n",
    "\n",
    "        self.instance_data_root = Path(instance_data_root)\n",
    "        if not self.instance_data_root.exists():\n",
    "            raise ValueError(f\"Instance {self.instance_data_root} images root doesn't exists.\")\n",
    "\n",
    "        self.instance_images_path = list(Path(instance_data_root).iterdir())\n",
    "        self.num_instance_images = len(self.instance_images_path)\n",
    "        self.instance_prompt = instance_prompt\n",
    "        self._length = self.num_instance_images\n",
    "\n",
    "        if class_data_root is not None:\n",
    "            self.class_data_root = Path(class_data_root)\n",
    "            self.class_data_root.mkdir(parents=True, exist_ok=True)\n",
    "            self.class_images_path = list(self.class_data_root.iterdir())\n",
    "            if class_num is not None:\n",
    "                self.num_class_images = min(len(self.class_images_path), class_num)\n",
    "            else:\n",
    "                self.num_class_images = len(self.class_images_path)\n",
    "            self._length = max(self.num_class_images, self.num_instance_images)\n",
    "            self.class_prompt = class_prompt\n",
    "        else:\n",
    "            self.class_data_root = None\n",
    "\n",
    "        self.image_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Load instance captions from HF dataset\n",
    "        self.instance_captions = None\n",
    "        if dataset_name:\n",
    "            from datasets import load_dataset\n",
    "            dataset = load_dataset(dataset_name)\n",
    "            self.instance_captions = dataset[\"train\"][text_field]\n",
    "            assert len(self.instance_captions) >= self.num_instance_images, \\\n",
    "                f\"Dataset has {len(self.instance_captions)} captions but {self.num_instance_images} images\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        instance_image = Image.open(self.instance_images_path[index % self.num_instance_images])\n",
    "        # instance_image = exif_transpose(instance_image)\n",
    "\n",
    "        if not instance_image.mode == \"RGB\":\n",
    "            instance_image = instance_image.convert(\"RGB\")\n",
    "        example[\"instance_images\"] = self.image_transforms(instance_image)\n",
    "\n",
    "        if self.encoder_hidden_states is not None:\n",
    "            example[\"instance_prompt_ids\"] = self.encoder_hidden_states\n",
    "        else:\n",
    "            # Store both concept and instance prompts\n",
    "            concept_inputs = torch.ones(1, 1, 768)\n",
    "            example[\"concept_prompt_ids\"] = concept_inputs.input_ids\n",
    "            example[\"concept_attention_mask\"] = concept_inputs.attention_mask\n",
    "            \n",
    "            if self.instance_captions is not None:\n",
    "                instance_inputs = tokenize_prompt(\n",
    "                    self.tokenizer, \n",
    "                    self.instance_captions[index % self.num_instance_images], \n",
    "                    tokenizer_max_length=self.tokenizer_max_length\n",
    "                )\n",
    "                example[\"instance_prompt_ids\"] = instance_inputs.input_ids\n",
    "                example[\"instance_attention_mask\"] = instance_inputs.attention_mask\n",
    "            else:\n",
    "                # If no instance captions, use concept prompt\n",
    "                example[\"instance_prompt_ids\"] = concept_inputs.input_ids\n",
    "                example[\"instance_attention_mask\"] = concept_inputs.attention_mask\n",
    "\n",
    "        if self.class_data_root:\n",
    "            class_image = Image.open(self.class_images_path[index % self.num_class_images])\n",
    "            # class_image = exif_transpose(class_image)\n",
    "\n",
    "            if not class_image.mode == \"RGB\":\n",
    "                class_image = class_image.convert(\"RGB\")\n",
    "            example[\"class_images\"] = self.image_transforms(class_image)\n",
    "\n",
    "            if self.class_prompt_encoder_hidden_states is not None:\n",
    "                example[\"class_prompt_ids\"] = self.class_prompt_encoder_hidden_states\n",
    "            else:\n",
    "                class_text_inputs = tokenize_prompt(\n",
    "                    self.tokenizer, self.class_prompt, tokenizer_max_length=self.tokenizer_max_length\n",
    "                )\n",
    "                example[\"class_prompt_ids\"] = class_text_inputs.input_ids\n",
    "                example[\"class_attention_mask\"] = class_text_inputs.attention_mask\n",
    "\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        dataset_name,\n",
    "        size=512,\n",
    "        center_crop=False,\n",
    "        encoder_hidden_states=None,\n",
    "        tokenizer_max_length=None,\n",
    "    ):\n",
    "        self.size = size\n",
    "        self.center_crop = center_crop\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoder_hidden_states = encoder_hidden_states\n",
    "        self.tokenizer_max_length = tokenizer_max_length\n",
    "\n",
    "        # Load dataset directly from Hugging Face\n",
    "        from datasets import load_dataset\n",
    "        self.dataset = load_dataset(dataset_name)[\"train\"]\n",
    "        self._length = len(self.dataset)\n",
    "\n",
    "        self.image_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        \n",
    "        # Get image directly from dataset\n",
    "        instance_image = self.dataset[index][\"image\"]\n",
    "        if not instance_image.mode == \"RGB\":\n",
    "            instance_image = instance_image.convert(\"RGB\")\n",
    "        example[\"instance_images\"] = self.image_transforms(instance_image)\n",
    "\n",
    "        if self.encoder_hidden_states is not None:\n",
    "            example[\"instance_prompt_ids\"] = self.encoder_hidden_states\n",
    "        else:\n",
    "            # Get caption directly from dataset\n",
    "            instance_inputs = tokenize_prompt(\n",
    "                self.tokenizer,\n",
    "                self.dataset[index][\"text\"],\n",
    "                tokenizer_max_length=self.tokenizer_max_length\n",
    "            )\n",
    "            example[\"instance_prompt_ids\"] = instance_inputs.input_ids\n",
    "            example[\"instance_attention_mask\"] = instance_inputs.attention_mask\n",
    "\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        dataset_name,\n",
    "        size=512,\n",
    "        center_crop=False,\n",
    "        encoder_hidden_states=None,\n",
    "        class_data_root=None,\n",
    "        class_prompt=None,\n",
    "        class_num=None,\n",
    "        class_prompt_encoder_hidden_states=None,\n",
    "        tokenizer_max_length=None,\n",
    "    ):\n",
    "        self.size = size\n",
    "        self.center_crop = center_crop\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoder_hidden_states = encoder_hidden_states\n",
    "        self.tokenizer_max_length = tokenizer_max_length\n",
    "        self.class_prompt_encoder_hidden_states = class_prompt_encoder_hidden_states\n",
    "\n",
    "        # Load dataset directly from Hugging Face\n",
    "        from datasets import load_dataset\n",
    "        self.dataset = load_dataset(dataset_name)[\"train\"]\n",
    "        self._length = len(self.dataset)\n",
    "\n",
    "        # Class image handling\n",
    "        if class_data_root is not None:\n",
    "            self.class_data_root = Path(class_data_root)\n",
    "            self.class_data_root.mkdir(parents=True, exist_ok=True)\n",
    "            self.class_images_path = list(self.class_data_root.iterdir())\n",
    "            if class_num is not None:\n",
    "                self.num_class_images = min(len(self.class_images_path), class_num)\n",
    "            else:\n",
    "                self.num_class_images = len(self.class_images_path)\n",
    "            self._length = max(self.num_class_images, len(self.dataset))\n",
    "            self.class_prompt = class_prompt\n",
    "        else:\n",
    "            self.class_data_root = None\n",
    "\n",
    "        self.image_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        \n",
    "        # Get instance image and text directly from dataset\n",
    "        instance_image = self.dataset[index % len(self.dataset)][\"image\"]\n",
    "        if not instance_image.mode == \"RGB\":\n",
    "            instance_image = instance_image.convert(\"RGB\")\n",
    "        example[\"instance_images\"] = self.image_transforms(instance_image)\n",
    "\n",
    "        if self.encoder_hidden_states is not None:\n",
    "            example[\"instance_prompt_ids\"] = self.encoder_hidden_states\n",
    "        else:\n",
    "            instance_inputs = tokenize_prompt(\n",
    "                self.tokenizer,\n",
    "                self.dataset[index % len(self.dataset)][\"text\"],\n",
    "                tokenizer_max_length=self.tokenizer_max_length\n",
    "            )\n",
    "            example[\"instance_prompt_ids\"] = instance_inputs.input_ids\n",
    "            example[\"instance_attention_mask\"] = instance_inputs.attention_mask\n",
    "\n",
    "        # Handle class images if provided\n",
    "        if self.class_data_root:\n",
    "            class_image = Image.open(self.class_images_path[index % self.num_class_images])\n",
    "            if not class_image.mode == \"RGB\":\n",
    "                class_image = class_image.convert(\"RGB\")\n",
    "            example[\"class_images\"] = self.image_transforms(class_image)\n",
    "\n",
    "            if self.class_prompt_encoder_hidden_states is not None:\n",
    "                example[\"class_prompt_ids\"] = self.class_prompt_encoder_hidden_states\n",
    "            else:\n",
    "                class_text_inputs = tokenize_prompt(\n",
    "                    self.tokenizer, \n",
    "                    self.class_prompt, \n",
    "                    tokenizer_max_length=self.tokenizer_max_length\n",
    "                )\n",
    "                example[\"class_prompt_ids\"] = class_text_inputs.input_ids\n",
    "                example[\"class_attention_mask\"] = class_text_inputs.attention_mask\n",
    "\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"tokenizer\")\n",
    "dataset = InceptionDataset(tokenizer, \"Ksgk-fy/concept-kanji-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
